{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a405b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051d425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_folder = \"../data/all/ori\"\n",
    "ann_folder = \"../data/all/ann\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a22d1b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174_Consult-HistoryandPhy.-FlankPain-Consult_8.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Alcohol',\n",
       " 'Amount',\n",
       " 'Drug',\n",
       " 'EnvironmentalExposure',\n",
       " 'ExposureHistory',\n",
       " 'Extent',\n",
       " 'Family',\n",
       " 'Frequency',\n",
       " 'History',\n",
       " 'InfectiousDiseases',\n",
       " 'LivingSituation',\n",
       " 'LivingStatus',\n",
       " 'Location',\n",
       " 'MaritalStatus',\n",
       " 'MedicalCondition',\n",
       " 'Method',\n",
       " 'Occupation',\n",
       " 'Other',\n",
       " 'PhysicalActivity',\n",
       " 'QuitHistory',\n",
       " 'Residence',\n",
       " 'SexualHistory',\n",
       " 'Status',\n",
       " 'Temporal',\n",
       " 'Tobacco',\n",
       " 'Type'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type = []\n",
    "T_cate = []\n",
    "for filename in os.listdir(ori_folder):\n",
    "    ori_filepath = os.path.join(ori_folder, filename)\n",
    "    ann_filepath = os.path.join(ann_folder, filename[:-3]+\"ann\")\n",
    "    ori_file = open(ann_filepath,encoding=\"utf-8\")\n",
    "\n",
    "    for line in open(ann_filepath, 'r', encoding='utf8'):\n",
    "        type.append(line[0])\n",
    "        if line[0]==\"#\":\n",
    "            print(filename)\n",
    "        if line.startswith(\"T\"):\n",
    "            T_cate.append(line.split(\"\\t\")[1].split()[0])\n",
    "    \n",
    "set(T_cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838f76c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 0, 4),\n",
       " ('is', 5, 7),\n",
       " ('a', 8, 9),\n",
       " ('sentence', 10, 18),\n",
       " ('.', 18, 19),\n",
       " (),\n",
       " ('Here', 20, 24),\n",
       " ('is', 25, 27),\n",
       " ('another', 28, 35),\n",
       " ('one', 36, 39),\n",
       " ('.', 39, 40),\n",
       " ()]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_with_positions(text):\n",
    "    # Sentence tokenize\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Word tokenize each sentence and find positions\n",
    "    tokenized_text = []\n",
    "    global_offset = 0  # Offset to track position in the entire text\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        offset = 0  # Offset to track position within the current sentence\n",
    "        for word in words:\n",
    "            # Find the position of the word in the sentence\n",
    "            match = re.search(re.escape(word), sentence[offset:])\n",
    "            if match:\n",
    "                start = global_offset + offset + match.start()\n",
    "                end = global_offset + offset + match.end()\n",
    "                # Store word with its position (start, end)\n",
    "                tokenized_text.append((word, start, end))\n",
    "                offset += match.end()\n",
    "        \n",
    "        global_offset += len(sentence) + 1  # +1 for the space or punctuation after the sentence\n",
    "        tokenized_text.append(())\n",
    "    return tokenized_text\n",
    "\n",
    "# Example usage\n",
    "paragraph = \"This is a sentence. Here is another one.\"\n",
    "word_positions = tokenize_with_positions(paragraph)\n",
    "word_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45450da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BratStandoffReader:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.entities = {}\n",
    "        self.relations = {}\n",
    "        self.attributes = {}\n",
    "        self.events = {}\n",
    "        self._read_file()\n",
    "\n",
    "    def _read_file(self):\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                self._parse_line(line.strip())\n",
    "\n",
    "    def _parse_line(self, line):\n",
    "        if line.startswith('T'):  # Text-bound annotation (Entity)\n",
    "            self._parse_entity(line)\n",
    "        elif line.startswith('R'):  # Relation annotation\n",
    "            self._parse_relation(line)\n",
    "        elif line.startswith('A'):  # Attribute annotation\n",
    "            self._parse_attribute(line)\n",
    "        elif line.startswith('E'):  # Event annotation\n",
    "            self._parse_event(line)\n",
    "\n",
    "    def _parse_entity(self, line):\n",
    "        parts = line.split('\\t')\n",
    "        entity_id = parts[0]\n",
    "        entity_info = parts[1].split()\n",
    "        label = entity_info[0]\n",
    "        start = int(entity_info[1])\n",
    "        end = int(entity_info[2])\n",
    "        self.entities[entity_id] = {'label': label, 'start': start, 'end': end, 'text': parts[2]}\n",
    "\n",
    "    def _parse_relation(self, line):\n",
    "        parts = line.split()\n",
    "        relation_id = parts[0]\n",
    "        label = parts[1]\n",
    "        arg1 = parts[2].split(':')[1]\n",
    "        arg2 = parts[3].split(':')[1]\n",
    "        self.relations[relation_id] = {'label': label, 'arg1': arg1, 'arg2': arg2}\n",
    "\n",
    "    def _parse_attribute(self, line):\n",
    "        parts = line.split()\n",
    "        attribute_id = parts[0]\n",
    "        label = parts[1]\n",
    "        entity_id = parts[2]\n",
    "        value = parts[3] if len(parts) > 3 else None\n",
    "        self.attributes[attribute_id] = {'label': label, 'entity': entity_id, 'value': value}\n",
    "        \n",
    "    def _parse_event(self, line):\n",
    "        parts = line.split()\n",
    "        event_id = parts[0]\n",
    "        event_info = parts[1].split(':')\n",
    "        trigger = event_info[1]\n",
    "        args = {part.split(':')[0]: part.split(':')[1] for part in parts[1:] if ':' in part}\n",
    "        self.events[event_id] = {'trigger': trigger, 'args': args}\n",
    "\n",
    "    def get_event(self, event_id):\n",
    "        return self.events.get(event_id)\n",
    "\n",
    "    def get_entity(self, entity_id):\n",
    "        return self.entities.get(entity_id)\n",
    "\n",
    "    def get_relation(self, relation_id):\n",
    "        return self.relations.get(relation_id)\n",
    "\n",
    "    def get_attribute(self, attribute_id):\n",
    "        return self.attributes.get(attribute_id)\n",
    "    \n",
    "    def get_pos_with_labelT(self):\n",
    "        pos_with_label = []\n",
    "        for i in self.entities:\n",
    "            pos_with_label.append((self.entities[i][\"label\"],\n",
    "                                   self.entities[i][\"start\"],\n",
    "                                   self.entities[i][\"end\"]))\n",
    "        return pos_with_label\n",
    "    \n",
    "    def get_label_T_layer_num(self,layer_num):\n",
    "        anno_list = self.get_pos_with_labelT()\n",
    "        mulyer_anno = []\n",
    "        processed = []\n",
    "\n",
    "        while len(processed)<len(anno_list):\n",
    "            layer = []\n",
    "            occupied=set([])\n",
    "            for i,a in enumerate(anno_list):\n",
    "                if i in processed:\n",
    "                    continue\n",
    "\n",
    "                if len(set(range(a[1],a[2])) & occupied)==0:\n",
    "\n",
    "                    occupied=set(range(a[1],a[2])) | occupied\n",
    "                    layer.append(a)\n",
    "                    processed.append(i)\n",
    "\n",
    "            mulyer_anno.append(layer)\n",
    "\n",
    "        if len(mulyer_anno)>0:\n",
    "            return mulyer_anno[layer_num]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "    def get_label_T_layer_list2(self):\n",
    "        anno_list = self.get_pos_with_labelT()\n",
    "        mulyer_anno = []\n",
    "        processed = []\n",
    "        while len(processed)<len(anno_list):\n",
    "            layer = []\n",
    "            current = 0\n",
    "            for i,a in enumerate(anno_list):\n",
    "                if i in processed:\n",
    "                    continue\n",
    "                if a[1]>current:\n",
    "                    current=a[2]\n",
    "                    layer.append(a)\n",
    "                    processed.append(i)\n",
    "\n",
    "            mulyer_anno.append(layer)\n",
    "\n",
    "        return mulyer_anno\n",
    "    \n",
    "    def get_label_T_layer_list(self):\n",
    "        anno_list = self.get_pos_with_labelT()\n",
    "        mulyer_anno = []\n",
    "        processed = []\n",
    "\n",
    "        while len(processed)<len(anno_list):\n",
    "            layer = []\n",
    "            occupied=set([])\n",
    "            for i,a in enumerate(anno_list):\n",
    "                if i in processed:\n",
    "                    continue\n",
    "\n",
    "                if len(set(range(a[1],a[2])) & occupied)==0:\n",
    "\n",
    "                    occupied=set(range(a[1],a[2])) | occupied\n",
    "                    layer.append(a)\n",
    "                    processed.append(i)\n",
    "\n",
    "            mulyer_anno.append(layer)\n",
    "\n",
    "        return mulyer_anno\n",
    "    \n",
    "    \n",
    "\n",
    "    def mun_of_layerT(self):\n",
    "        return len(self. get_label_T_layer_list())\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1bc3571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def single_layer_data(ori_folder,ann_folder,output_path):\n",
    "    all_token_merged = open(output_path,\"w\",encoding=\"utf-8\")\n",
    "    for filename in os.listdir(ori_folder):\n",
    "        ori_filepath = ori_folder+\"/\"+filename\n",
    "        ann_filepath = ann_folder+\"/\"+filename[:-3]+\"ann\"\n",
    "        with open(ori_filepath, 'r', encoding='utf8') as f:\n",
    "            ori_sents = f.read().strip()\n",
    "            tokens = tokenize_with_positions(ori_sents)\n",
    "\n",
    "        annos = BratStandoffReader(ann_filepath)\n",
    "        anno_list = annos.get_label_T_layer_num(0)\n",
    "\n",
    "        for token in tokens:\n",
    "            if len(token)==0:\n",
    "                all_token_merged.write(\"\\n\")\n",
    "            else:\n",
    "                label = \"\"\n",
    "                word = token[0]\n",
    "                w_start = token[1]\n",
    "                w_end = token[2]\n",
    "\n",
    "                for anno in anno_list:\n",
    "                    ann = anno[0]\n",
    "                    a_start = anno[1]\n",
    "                    a_end = anno[2]\n",
    "\n",
    "                    if w_start == a_start:\n",
    "                        label += \"B-\"+ann\n",
    "                    elif w_start > a_start and w_end<a_end:\n",
    "                        label = \"I-\"+ann\n",
    "                    elif w_end==a_end:\n",
    "                        label = \"I-\"+ann\n",
    "                if len(label)<1:\n",
    "                    label = \"O\"\n",
    "                if label.count(\"-\")>1:\n",
    "                    label = \"O\"\n",
    "\n",
    "                all_token_merged.write(word +\" \"+ label+\"\\n\")    \n",
    "    all_token_merged.close()\n",
    "    return anno_list\n",
    "\n",
    "\n",
    "ori_folder = \"../data/train/ori\"\n",
    "ann_folder = \"../data/train/ann\"\n",
    "output_path = \"../data/train/training_data/train_single_layer.txt\"\n",
    "single_layer_data(ori_folder,ann_folder,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ec2ef9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_folder = \"../data/test/ori\"\n",
    "ann_folder = \"../data/test/ann\"\n",
    "output_path = \"../data/test/test_data/test_single_layer.txt\"\n",
    "single_layer_data(ori_folder,ann_folder,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc34f1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MaritalStatus', 18, 25),\n",
       " ('Status', 27, 36),\n",
       " ('Status', 42, 50),\n",
       " ('Alcohol', 51, 66),\n",
       " ('Status', 68, 81),\n",
       " ('Type', 82, 94),\n",
       " ('Drug', 95, 104)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_folder = \"../data/valid/ori\"\n",
    "ann_folder = \"../data/valid/ann\"\n",
    "output_path = \"../data/valid/valid_data/valid_single_layer.txt\"\n",
    "single_layer_data(ori_folder,ann_folder,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffb3dc46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mulit_layer_data(ori_folder,ann_folder,output_path):\n",
    "    all_token_merged = open(output_path,\"w\",encoding=\"utf-8\")\n",
    "    for filename in os.listdir(ori_folder):\n",
    "        ori_filepath = ori_folder+\"/\"+filename\n",
    "        ann_filepath = ann_folder+\"/\"+filename[:-3]+\"ann\"\n",
    "        with open(ori_filepath, 'r', encoding='utf8') as f:\n",
    "            ori_sents = f.read().strip()\n",
    "            tokens = tokenize_with_positions(ori_sents)\n",
    "        \n",
    "\n",
    "        annos = BratStandoffReader(ann_filepath)\n",
    "        anno_list = annos.get_label_T_layer_num(0)\n",
    "        multi_anno = annos.get_label_T_layer_list()\n",
    "        #print(ori_filepath)\n",
    "        for token in tokens:\n",
    "            if len(token)==0:\n",
    "                all_token_merged.write(\"\\n\")\n",
    "                #print()\n",
    "\n",
    "            else:\n",
    "                all_label = \"\"\n",
    "                for anno_list in multi_anno:\n",
    "                    word = token[0]\n",
    "                    w_start = token[1]\n",
    "                    w_end = token[2]\n",
    "                    label = \"\"\n",
    "\n",
    "                    for anno in anno_list:\n",
    "                        ann = anno[0]\n",
    "                        a_start = anno[1]\n",
    "                        a_end = anno[2]\n",
    "\n",
    "                        if w_start == a_start:\n",
    "                            label += \"B-\"+ann+\" \"\n",
    "                        elif w_start > a_start and w_end<a_end:\n",
    "                            label = \"I-\"+ann+\" \"\n",
    "                        elif w_end==a_end:\n",
    "                            label = \"I-\"+ann+\" \"\n",
    "                    if len(label)<1:\n",
    "                        label = \"O \"\n",
    "                    all_label+=label\n",
    "\n",
    "                \n",
    "                \n",
    "                if len(all_label)>0:\n",
    "                    #print(word +\" \"+ all_label)\n",
    "                    all_token_merged.write(word +\" \"+ all_label+\"\\n\")\n",
    "    all_token_merged.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ori_folder = \"../data/train/ori\"\n",
    "ann_folder = \"../data/train/ann\"\n",
    "output_path = \"../data/train/training_data/train_multi_layer.txt\"\n",
    "mulit_layer_data(ori_folder,ann_folder,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46992149",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_folder = \"../data/test/ori\"\n",
    "ann_folder = \"../data/test/ann\"\n",
    "output_path = \"../data/test/test_data/test_multi_layer.txt\"\n",
    "mulit_layer_data(ori_folder,ann_folder,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d6a5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_folder = \"../data/valid/ori\"\n",
    "ann_folder = \"../data/valid/ann\"\n",
    "output_path = \"../data/valid/valid_data/valid_multi_layer.txt\"\n",
    "mulit_layer_data(ori_folder,ann_folder,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52afeaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b58e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3433b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
